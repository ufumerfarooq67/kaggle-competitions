---
title: '  Kaggle American Housing Dataset'

date: "08 May 2021"
output:
  html_document:
    number_sections: yes
    toc: yes
    theme: readable
    highlight: haddock
  pdf_document:
    toc: yes
---

**Introduction**
This project intends to dive deep into the trends of sale price in american housing dataset  using cutting edge technologies. These insights will assist us in pinpointing
the core root variables effecting sale price of houses.    

**Summary** (probelm analysis and finding)
This dataset aims to predict prices of houses based on various variables presented in the data set and thus help in future prediction.

**Dataset**

We acquired the data set from kaggle. Name of the dataset is House Prices - Advanced Regression Techniques. https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data. 




```{r }
library(dplyr)
library(tidyr)
library(ModelMetrics)
library(Metrics)
library(corrplot)
library(caTools)  # loading caTools library
library(ggplot2)
library(scales)
library(glmnet)
library(caret)
library(rpart) # significant trends within countries
library(rpart.plot)
library(randomForest)
theme_set(theme_light())

# 1) Import & data cleaning

training <- read.csv("train.csv")
testing <- read.csv("test.csv")
sample_submission <- read.csv("sample_submission.csv")
```
# Problem Identification
A data scientist all alone cannot analyse and formulate the solution for a problem. The best solutions are formulated when domain expertise join hands with data skills. The ultimate objective will be to analyze and predict the pattrens of house prices. Multiple data science skills which will be used here will be, data analysis, data pre-processing, Pinpointing valuable feature columns and machine learning models selections and implementation. Whereas we cannot conquer the problem only with these skillsets, we will need domain expertise like what facilities people prefer which buying a house in a certain range and what price range is commonly demanded by the buyers. The seasons in which buyers go out for buying expensive and cheap houses. Without these deep insights, data cannot do much. 
```{r }
# Combining both the dataset for data exploration and pre-processing.
output_train <- training['SalePrice']
data <-  rbind(within(training, rm('SalePrice')), testing)

#1 information on data set
# Dimension of dataset
dim(data)

# Structure of dataset
str(data)



```
Questions of interest for Problem Identification and understanding. 

* 8 Questions For Data Analysis. Questions of Interest (QOI):
  + QOI 1. Which variable has most effect on sale price?
  + QOI 2. What is the strength of relationship between GrLivArea and sale price?
  + QOI 3. Which model is best among the models used?
  + QOI 4. What is RMSE value of best performing model? Can it be reduced further?
  + QOI 5. What is the correlation type of overallQual with sale price.
  + QOI 6. Which evaluation matrix should be used for evaluation and why?
  + QOI 7. Why the superior performimg model is most superior to others?
  + QOI 8. Which sale price range has maximum number of properties?

# Pre processing of Data and treating missing values
Pre-processing is the transformation applied to our raw data before feeding it to the algorithm. The raw data after transformation is converted into a cleaned dataset, this raw dataset plays the role of an obstacle in achieving better accuracies.
For achieving better results from the Machine Learning projects the format of the data has to be in a proper format. Some of the Machine Learning algorithms need information in a specified format, for example, Random Forest and decision tree algorithm do not support null values, therefore to execute the random forest and random forest algorithm null values have to be taken care of. Another application is to provide as much quality data as possible. By replacing null values with mean, mode or even by predicting the null values we can provide the machine learning algorithm with better insight into the problem. 
As discussed earlier we can use multiple techniques for pre-processing the data. We can just delete the null or empty values, other than that we can fill the n/a values with mean or mode. Or we can use regression, clustering techniques for predicting the empty values of the dataset. This seems to be the ideal solution for filling the empty tuples because if the algorithm is efficient enough, we can fill the dataset with the most realistic values.
```{r}

# Let find out if we have got null values.
any(is.na(data))

summary(data)


highlyCorrelatedfeaturesMethod <- function(dataset,val)
{
  train_lasso<- dataset[,sapply(dataset,is.numeric) | sapply(dataset,is.integer)]
  corr <- cor(train_lasso)
  correlation_salePrice <-tail(corr,1)
  correlation_salePrice <- correlation_salePrice[,colSums(correlation_salePrice) > val]
  correlation_salePrice <- data.frame(t(correlation_salePrice))
  correlation_salePrice
  strong_relation_names <- colnames(correlation_salePrice)
  strong_relation_names <- strong_relation_names[-15]
  strong_relation_names
  
  return (strong_relation_names)
}

# Cleaning, Pre-processing and Casting
preprocessing <- function(dataset)
{
  
  print("Cleaning the dataset and Pre-processing")
  # Let find those columns with null values.
  columns_null <- dataset[,colSums(is.na(dataset)) > 0]
  colnames(columns_null)
  
  # Categorical Columns with NA as None (No Type)
  columns_null$PoolQC <- columns_null$PoolQC %>% 
    replace_na("None")
  
  columns_null$MiscFeature <- columns_null$MiscFeature %>% 
    replace_na("None")
  
  columns_null$Alley <- columns_null$Alley %>% 
    replace_na("None")
  
  columns_null$Fence <- columns_null$Fence %>% 
    replace_na("None")
  
  columns_null$FireplaceQu <- columns_null$FireplaceQu %>% 
    replace_na("None")
  
  columns_null$GarageFinish <- columns_null$GarageFinish %>% 
    replace_na("None")
  
  columns_null$GarageQual <- columns_null$GarageQual %>% 
    replace_na("None")
  
  columns_null$GarageCond <- columns_null$GarageCond %>% 
    replace_na("None")
  
  columns_null$GarageType <- columns_null$GarageType %>% 
    replace_na("None")
  
  columns_null$BsmtCond <- columns_null$BsmtCond %>% 
    replace_na("None")
  
  columns_null$BsmtExposure <- columns_null$BsmtExposure %>% 
    replace_na("None")
  
  columns_null$BsmtQual <- columns_null$BsmtQual %>% 
    replace_na("None")
  
  columns_null$BsmtFinType2 <- columns_null$BsmtFinType2 %>% 
    replace_na("None")
  
  columns_null$BsmtFinType1 <- columns_null$BsmtFinType1 %>% 
    replace_na("None")
  
  columns_null$MasVnrType <- columns_null$MasVnrType %>% 
    replace_na("None")
  
  
  # Categorical Columns with no clear NA meaning (replacing with value having highest frequency)
  columns_null$MSZoning <- columns_null$MSZoning %>% 
    replace_na(tail(names(sort(table(columns_null$MSZoning))), 1))
  
  
  columns_null$Utilities <- columns_null$Utilities %>% 
    replace_na(tail(names(sort(table(columns_null$Utilities))), 1))
  
  
  columns_null$Functional <- columns_null$Functional %>% 
    replace_na(tail(names(sort(table(columns_null$Functional))), 1))
  
  
  columns_null$Exterior1st <- columns_null$Exterior1st %>% 
    replace_na(tail(names(sort(table(columns_null$Exterior1st))), 1))
  
  
  columns_null$Exterior2nd <- columns_null$Exterior2nd %>% 
    replace_na(tail(names(sort(table(columns_null$Exterior2nd))), 1))
  
  
  columns_null$Electrical <- columns_null$Electrical %>% 
    replace_na(tail(names(sort(table(columns_null$Electrical))), 1))
  
  
  columns_null$KitchenQual <- columns_null$KitchenQual %>% 
    replace_na(tail(names(sort(table(columns_null$KitchenQual))), 1))
  
  columns_null$SaleType <- columns_null$SaleType %>% 
    replace_na(tail(names(sort(table(columns_null$SaleType))), 1))
  
  
  
  #########################################
  
  
  # Numerical Columns (Replace NA with mean)
  
  columns_null$LotFrontage <- columns_null$LotFrontage %>% 
    replace_na(mean(columns_null$LotFrontage,na.rm=TRUE))
  
  
  columns_null$GarageYrBlt <- columns_null$GarageYrBlt %>% 
    replace_na(round(mean(columns_null$GarageYrBlt,na.rm = TRUE)))
  
  columns_null$GarageCars <- columns_null$GarageCars %>% 
    replace_na(round(mean(columns_null$GarageCars,na.rm = TRUE)))
  
  columns_null$GarageArea <- columns_null$GarageArea %>% 
    replace_na(round(mean(columns_null$GarageArea,na.rm = TRUE)))
  
  
  columns_null$BsmtFullBath <- columns_null$BsmtFullBath %>% 
    replace_na(round(mean(columns_null$BsmtFullBath,na.rm = TRUE)))
  
  
  columns_null$BsmtHalfBath <- columns_null$BsmtHalfBath %>% 
    replace_na(round(mean(columns_null$BsmtHalfBath,na.rm = TRUE)))
  
  
  columns_null$BsmtFinSF1 <- columns_null$BsmtFinSF1 %>% 
    replace_na(round(mean(columns_null$BsmtFinSF1,na.rm = TRUE)))
  
  
  columns_null$BsmtFinSF2 <- columns_null$BsmtFinSF2 %>% 
    replace_na(round(mean(columns_null$BsmtFinSF2,na.rm = TRUE)))
  
  
  columns_null$BsmtUnfSF <- columns_null$BsmtUnfSF %>% 
    replace_na(round(mean(columns_null$BsmtUnfSF,na.rm = TRUE)))
  
  
  columns_null$TotalBsmtSF <- columns_null$TotalBsmtSF %>% 
    replace_na(round(mean(columns_null$TotalBsmtSF,na.rm = TRUE)))
  
  
  columns_null$MasVnrArea <- columns_null$MasVnrArea %>% 
    replace_na(round(mean(columns_null$MasVnrArea,na.rm = TRUE)))
  
  
  # Let's replace new columns without any NA values.
  dataset[,colnames(columns_null)] <- columns_null
  
  columns_null <- dataset[,colSums(is.na(dataset)) > 0]
  colnames(columns_null)
  # No, null values
  
  any(is.na(dataset))
# False mean, not even a single value is null.
  
  
  
  #####################################################################
  
  ########## CHANGING THE TYPES OF PARTICULAR COLUMNS
  
  ####################################################################
  
  # Changing categorical with factor type to numeric
  
  #Categorical factors that should be numeric
  dataset$ExterQual <- as.numeric(factor(dataset$ExterQual))
  dataset$ExterCond <- as.numeric(factor(dataset$ExterCond))
  dataset$BsmtQual <- as.numeric(factor(dataset$BsmtQual))
  dataset$BsmtCond <- as.numeric(factor(dataset$BsmtCond))
  dataset$BsmtExposure <- as.numeric(factor(dataset$BsmtExposure))
  dataset$BsmtFinType1 <- as.numeric(factor(dataset$BsmtFinType1))
  dataset$BsmtFinType2 <- as.numeric(factor(dataset$BsmtFinType2))
  dataset$HeatingQC <- as.numeric(factor(dataset$HeatingQC))
  dataset$KitchenQual <- as.numeric(factor(dataset$KitchenQual))
  dataset$FireplaceQu <- as.numeric(factor(dataset$FireplaceQu))
  dataset$GarageQual <- as.numeric(factor(dataset$GarageQual))
  dataset$GarageCond <- as.numeric(factor(dataset$GarageCond))
  dataset$PoolQC <- as.numeric(factor(dataset$PoolQC))
  dataset$Condition2 <- as.numeric(factor(dataset$Condition2))
  dataset$Heating <- as.numeric(factor(dataset$Heating))

  
  # Changing few numeric to factor
  dataset$MoSold <- factor(dataset$MoSold)
  dataset$MSSubClass <- factor(dataset$MSSubClass)
  
  
  
  # New Feature Generation
  ###Combining existing features###
  dataset$OverallGrade <- dataset$OverallQual * dataset$OverallCond
  dataset$GarageGrade <- dataset$GarageQual * dataset$GarageCond
  dataset$ExterGrade <- dataset$ExterQual * dataset$ExterCond
  dataset$KitchenScore <- dataset$KitchenAbvGr * dataset$KitchenQual
  dataset$FireplaceScore <- dataset$Fireplaces * dataset$FireplaceQu
  dataset$GarageScore <- dataset$GarageArea * dataset$GarageQual
  dataset$PoolScore <- dataset$PoolArea * dataset$PoolArea
  dataset$TotalBath <- dataset$BsmtFullBath + (0.5 * dataset$BsmtHalfBath) + dataset$FullBath + (0.5 * dataset$HalfBath)
  dataset$TotalSF <- dataset$TotalBsmtSF + dataset$X1stFlrSF + dataset$X2ndFlrSF
  dataset$TotalPorchSF <- dataset$OpenPorchSF + dataset$EnclosedPorch + dataset$X3SsnPorch + dataset$ScreenPorch

  
  return (dataset)
}

```

***Key Insights***

* LotFrontage has the maximum NA values i.e 486. All these NA values are eithered removed or replaced by mean value thriugh preprocessing of data. 

## What have we done here in pre-processing? 
We have used multiple techniques for the prediction of null values in this dataset. The categorical columns which were null were replaced with "none" keywords. Some of the other categorical values were replaced with the most frequent categories. Other then these, all the numerical values were replaced with mean.


# MODELLING
Use of different models
***Training ,Testing Data set and Prediction***

```{r}
dataset = preprocessing(data)
n <- nrow(testing)

train<-dataset[1:nrow(training),]
test <- dataset[nrow(training)+1:n,]

train <- cbind(train,output_train)


# XGBOOST
library(xgboost)
################################
head(train)
train_xgboost <- train[,c("GrLivArea","SalePrice")]

set.seed(123)   
sample = sample.split(train_xgboost,SplitRatio = 0.70) 
newTrain =subset(train_xgboost,sample == TRUE) 
newTest=subset(train_xgboost, sample == FALSE)

param <- list(colsample_bytree = 1,
              subsample = .6,
              booster = "gbtree",
              max_depth = 8,
              eta = 0.05,
              min_child_weight = 2,
              eval_metric = "rmse",
              objective="reg:linear",
              gamma = 0.01)


dtrain = xgb.DMatrix(as.matrix(sapply(newTrain, as.numeric)), label=newTrain$SalePrice)
dtest = xgb.DMatrix(as.matrix(sapply(newTest, as.numeric)), label=newTest$SalePrice)

model <- xgb.train(params = param,
            data = dtrain,
            nrounds = 4000,
            watchlist = list(train = dtrain),
            verbose = TRUE,
            print_every_n = 50,
            nthread = 2)


prediction <- predict(model, dtest)

prediction <- as.data.frame(as.matrix(prediction))
colnames(prediction) <- "prediction"

model_output <- cbind(newTest[2], prediction)

cor(model_output)
```



Random Forest performed the best because it uses many decision trees to predict each test row and this makes it an ensemble algorithm. In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.


```{r}
train_xgboost_2<- train[,sapply(train,is.numeric) | sapply(train,is.integer)]
corr <- cor(train_xgboost_2)
correlation_salePrice <-tail(corr,1)
correlation_salePrice <- correlation_salePrice[,colSums(correlation_salePrice) > 0.5]
correlation_salePrice <- data.frame(t(correlation_salePrice))
correlation_salePrice
strong_relation_names <- colnames(correlation_salePrice)
strong_relation_names <- strong_relation_names[-15]
strong_relation_names

features <-highlyCorrelatedfeaturesMethod(train,0.5)
# Training on whole dataset and test on testing dataset
train_xbg <- train[,c(features,"SalePrice")]
test_xbg <- test[,features]
test_xbg$SalePrice <- 0
colnames(train_xbg)
colnames(test_xbg)

param <- list(colsample_bytree = 1,
              subsample = .6,
              booster = "gbtree",
              max_depth = 8,
              eta = 0.08,
              min_child_weight = 2,
              eval_metric = "rmse",
              objective="reg:linear",
              gamma = 0.01)


dtrain = xgb.DMatrix(as.matrix(sapply(train_xbg, as.numeric)), label=train_xbg$SalePrice)
dtest = xgb.DMatrix(as.matrix(sapply(test_xbg, as.numeric)))

model <- xgb.train(params = param,
                   data = dtrain,
                   nrounds = 2000,
                   watchlist = list(train = dtrain),
                   verbose = TRUE,
                   print_every_n = 50,
                   nthread = 2)

prediction <- predict(model, dtest)

prediction <- as.data.frame(as.matrix(prediction))
colnames(prediction) <- "prediction"


model_output <- cbind(test$Id, prediction)
colnames(model_output) <- c("Id","SalePrice")

#write.csv(model_output,file="sample_submission.csv")
write.table(model_output,'xgbsubmission.csv',row.names=F, sep = ',')

```

XGBoost is one of the implementations of Gradient Boosting concept, but what makes XGBoost unique is that it uses a more regularized model formalization to control over-fitting, which gives it better performance. We have used XGBoost for predictions, and it is giving accuracy a bit lesser then the Random Forest algorithm.

```{r}
# Lasso Regression with Training Data
#####################################

# Training on whole dataset and test on testing dataset
lasso_data <- train[,c(features,"SalePrice")]

set.seed(123)   
sample = sample.split(lasso_data,SplitRatio = 0.70) 
newTrain =subset(lasso_data,sample == TRUE) 
newTest=subset(lasso_data, sample == FALSE)


set.seed(123)
control = trainControl(method ="cv", number = 5)
Grid_la_reg = expand.grid(alpha = 1,
                          lambda = seq(0.001, 0.1, by = 0.0002))

lasso_model = train(x = newTrain[,-15],
                    y = newTrain[,15],
                    method = "glmnet",
                    trControl = control,
                    tuneGrid = Grid_la_reg
)

# mean validation score
mean(lasso_model$resample$RMSE)


prediction_lasso <- predict(lasso_model,newTest[,-15])
prediction_lasso <- as.data.frame(as.matrix(prediction_lasso))

model_output <- data.frame(cbind(newTest[,15], prediction_lasso))
colnames(model_output) <- c("SalePrice","prediction")
head(model_output)
cor(model_output)

ggplot(data=model_output, aes(x = SalePrice, y = prediction, size = SalePrice-prediction)) +
  geom_point() +
  scale_x_continuous(breaks= seq(0, 1000000, by=100000),labels=comma) +
  scale_y_continuous(breaks= seq(0, 1000000, by=100000),labels=comma)

# Lasso Regression Model with Original Data
########################
# Training lasso regression model
set.seed(123)
control = trainControl(method ="cv", number = 5)
Grid_la_reg = expand.grid(alpha = 1,
                          lambda = seq(0.001, 0.1, by = 0.0002))

lasso_model = train(x = train_xbg[,-15],
                    y = train_xbg[,15],
                    method = "glmnet",
                    trControl = control,
                    tuneGrid = Grid_la_reg
)


# mean validation score
mean(lasso_model$resample$RMSE)




prediction_lasso <- predict(lasso_model,train_xbg[-1460,-15])

prediction_lasso <- as.data.frame(as.matrix(prediction_lasso))
colnames(prediction_lasso) <- "prediction"


model_output_ls <- cbind(test$Id, prediction_lasso)
colnames(model_output_ls) <- c("Id","SalePrice")
#write.csv(model_output_ls,file="sample_submission_ls.csv")
write.table(model_output_ls,"lasso_submission.csv",row.names=F, sep = ',')
```


```{r}
library(randomForest)

rf <- randomForest(
  formula = SalePrice ~ .,
  data    = train_xbg,
  ntree   = 1000,
  xtest = test_xbg[-15]
)


prediction_rf <- data.frame(rf$predicted)
model_output_rf <- cbind(test$Id, prediction_rf[-1460,])



colnames(model_output_rf) <- c("Id","SalePrice")

write.table(model_output_rf,'random_forest_submission.csv',row.names=F, sep = ',')
```

Random forest is the best used model. This information is obtained through RMSE valuE which is approximately 0.5


# Exploratory Data Analysis
Here we will be using visualizations and other analysis techniques for EOA.
We intended to understand the patterns in the sale price. For this we used visualization techniques, correlations and machine learning models for understanding  Which variables are most important and vice versa.


## Which variable has most effect on sale price?
From the below visualization we can interpret which feature columns have the most impact on the price. The impact is plot in descending order.
```{r}
set.seed(2018)
quick_RF <- randomForest(x=train[1:1460, -79], y=train$SalePrice[1:1460], ntree=100,importance=TRUE)
imp_RF <- importance(quick_RF)
imp_DF <- data.frame(Variables = row.names(imp_RF), MSE = imp_RF[,1])
imp_DF <- imp_DF[order(imp_DF$MSE, decreasing = TRUE),]

ggplot(imp_DF[1:20,], aes(x=reorder(Variables, MSE), y=MSE, fill=MSE)) + geom_bar(stat = 'identity') + labs(x = 'Variables', y= '% increase MSE if variable is randomly permuted') + coord_flip() + theme(legend.position="none")
```


## Which sale price range has maximum number of properties?
The range between 100,000 and 200,000 have the most properties.
```{r}
ggplot(data=train, aes(x=SalePrice)) +
  geom_histogram(fill="purple", binwidth = 10000) +
  scale_x_continuous(breaks= seq(0, 1000000, by=100000),labels=comma)
```

#Insights
As the sale price increases, the count of houses decreases which shows that maximum houses with high sale prices lie in range of 100,000 to 200,000

# Further Preprocessing

## What is the strength of relationship between  GrLivArea and sale price?
The following scatter plot shows the positive correlation between the two feature columns, This means as the one feature column increases the other increases too.
```{r}

ggplot(data=train, aes(x=GrLivArea,y=SalePrice)) +
  geom_point(col='purple') +
  scale_y_continuous(breaks= seq(0, 1000000, by=100000),labels=comma)
```


## What is the nature of correlation between overallQual and sale price?
It has positive correaltion as can be visualized by the graph.
```{r}
ggplot(data=train, aes(x=OverallQual,y=SalePrice)) +
  geom_point(col='brown') +
  scale_y_continuous(breaks= seq(0, 1000000, by=100000),labels=comma)
```
Following are the visualizations of relationship of many other feature columns with price. All these visualizations display unqiue insight into the complex data.

```{r}
ggplot(data=train, aes(x=YearBuilt,y=SalePrice)) +
 geom_point(col='yellow') +
  scale_y_continuous(breaks= seq(0, 1000000, by=100000),labels=comma)
```

```{r}
ggplot(data=train, aes(x=YearRemodAdd,y=SalePrice)) +
  geom_point(col='green') +
  scale_y_continuous(breaks= seq(0, 1000000, by=100000),labels=comma)
```

```{r}
ggplot(data=train, aes(x=TotalBsmtSF,y=SalePrice)) +
  geom_point(col='violet') +
  scale_y_continuous(breaks= seq(0, 1000000, by=100000),labels=comma)
```

```{r}
ggplot(data=train, aes(x=GrLivArea,y=SalePrice)) +
  geom_point(col='orange') +
  scale_y_continuous(breaks= seq(0, 1000000, by=100000),labels=comma)
```

```{r}
ggplot(data=train, aes(x=GarageCars,y=SalePrice)) +
  geom_point(col='purple') +
  scale_y_continuous(breaks= seq(0, 1000000, by=100000),labels=comma)
```

```{r}
ggplot(data=train, aes(x=TotalSF,y=SalePrice)) +
  geom_point(col='grey') +
  scale_y_continuous(breaks= seq(0, 1000000, by=100000),labels=comma)

```

```{r}
# Categorical

# MiscFeature as None is dominating feature in the market.
# Hence, we can use it as price predictor.
ggplot(data=train, aes(x=MiscFeature,y=SalePrice)) +
  geom_point(col='brown') +
  scale_y_continuous(breaks= seq(0, 1000000, by=100000),labels=comma)

# Houses with PavedDrives as Y (means Yes) are more dominating in the market.
# Can be used an idea price predictor.
ggplot(data=train, aes(x=PavedDrive,y=SalePrice)) +
   geom_point(col='yellow') +
  scale_y_continuous(breaks= seq(0, 1000000, by=100000),labels=comma)

# House with Electical as SBrkr are more dominating.
ggplot(data=train, aes(x=Electrical,y=SalePrice)) +
  geom_point(col='blue') +
  scale_y_continuous(breaks= seq(0, 1000000, by=100000),labels=comma)
```

## HeatMap
```{r}
# Features has the names of highly correlated columns with price
x  <- data.matrix(train[,features])
head(x)

library(caret)
preproc1 <- preProcess(x, method=c("center", "scale"))
norm1 <- predict(preproc1,x)
summary(norm1)
heatmap(x, Rowv=NA, Colv=NA, col = heat.colors(256), scale="column", margins=c(5,10))

```

## Boxplots
```{r}

x_new <- data.frame(data.matrix(train[,features]))
all_feature_names <- colnames(x)

for (name in all_feature_names) 
{
  n <- paste0(name,".jpg")
  jpeg(n, width = 350, height = 350)

  boxplot(x_new[,name])
  
  dev.off()
}


```

## Linear Regression and Residual Plot

```{r}

newData <- train[,c(features,"SalePrice")]

set.seed(123)   
sample = sample.split(newData,SplitRatio = 0.70) 
newTrain =subset(newData,sample == TRUE) 
newTest=subset(newData, sample == FALSE)
lmodel <- lm(data=newTrain,formula = SalePrice ~ .)

colnames(newTest)
prediction <- predict(lmodel,newTest[-15])
plot(lmodel$residuals)
abline(lmodel)

```

# EVALUATION
For the evaluation of machine learning algorithms, we needed to have standard metrics. So, we can compare the outcome of each algorithm and compare it with the actual labels of the data.
There are many evaluation metrics like MSE and RMSE. Here we have used the RMSE metric because it is the most commonly used and reliable evaluation metric. RMSE has the benefit of penalizing large errors more so can be more appropriate in some cases, for example, if being off by 10 is more than twice as bad as being off by 5. We can use it.
```{r}
val<-c(1.5,0.8,0.5)
barplot(val,names.arg = c("XGBoost", "Lasso Regress", "Random Forest"), col = "light green", main="RMSE Comparison")
```

## Which model is best among the models used?
It can be observed from the above plot that Random Forest is the best machine learning model with RMSE value of 0.5 

## Why the superior performimg model is most superior to others?
Random Forest uses hundreds and thousands of decision trees to predict each test tuple, which increases the likelihood of accurate prediction by many folds. These type of algorithms are known as ensemble algorithms because they use other algorithms in a way which boosts the accuracy.   

## Which evaluation matrix should be used for evaluation and why?
As discussed earlier RMSE value should be used for evaluation because it unearths the flawed approach of model if it is predicting values with much greater gap. This penalty is not involved in other evaluation metrices.

## What is RMSE value of best performing model? Can it be reduced further?

The accuracy of best performing model is 0.5. We have learned that accuracies of machine learning algorithm used could have increased if we would have pre processed the data well and if we would have used hyper perimeter tunning for each algorithm.
Following is the residual plot of Lasso Regression.
```{r}
plot(lasso_model, main = "Lasso Regression")
```

# Recommendations and Final Conclusions

These regression models have provided us with varied RMSE values. 
From these results it can be concluded that ensemble based Random forest regression models are best fit for these kind of data sets. Data set was raw in the begining as it had many null values. We used pre processing first, then we went for machine learning algorithms training and predictions. We have learned that accuracies of every machine learning algorithm used could have increased if we would have pre processed the data further. We can conclude the quality of data matters and after that selection of algorithm matters too. 

# References
* URL Links
  + https://www.rdocumentation.org/packages/randomForest/versions/4.6-14/topics/randomForest
  + https://cran.r-project.org/web/packages/glmnet/glmnet.pdf
  + https://www.rdocumentation.org/packages/xgboost/versions/0.4-4/topics/xgboost